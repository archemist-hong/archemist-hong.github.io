---
layout: single
title : "1.뉴런"
author_profile: true
categories: DL
tag: [python, pytorch, deeplearning]
toc: true
use_math: true
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }
    
    table.dataframe td {
      text-align: center;
      padding: 8px;
    }
    
    table.dataframe tr:hover {
      background: #b8d1f3; 
    }
    
    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# 뉴런


인공 신경망의 기본단위인 뉴런은 [Affine Transformation](https://luv-n-interest.tistory.com/810)과 Activation function으로 구성된다.



하나의 뉴런은 weight 와 bias 라는 parameter를 갖는다.

![그림1](../../images/2022-04-01-Neuron/그림1.png)


Activation function

- Sigmoid (정보 이론에서 logit을 probability로 변환시켜주는 역할)

- Tanh

- ReLU 등



Sigmoid를 사용하는 경우

\\(a = {1 \over (1 + exp^{-z})}\\)


## x가 1개 이고, 뉴런도 1개인 경우


Affine transformation

- \\(z = xw +b\\)


## x가 p개의 변수로 구성되고(다변량), 뉴런이 1개인 경우

![KakaoTalk_20220401_212929750_03](../../images/2022-04-01-Neuron/KakaoTalk_20220401_212929750_03-16488189871074.jpg)

## 다변량 x가 여러개이고(sample size가 n개), 뉴런이 1개인 경우

![KakaoTalk_20220401_212929750_04](../../images/2022-04-01-Neuron/KakaoTalk_20220401_212929750_04.jpg)

# 실습


## x가 1개 이고, 뉴런도 1개인 경우


### pytorch


```python

import torch



x = torch.tensor([[10.]])



Linear = torch.nn.Linear(in_features = 1, out_features = 1)

Sigmoid = torch.nn.Sigmoid()



y = Sigmoid(Linear(x))



W = Linear.weight

B = Linear.bias



print(f'y(Pytorch) : {y.detach().numpy()}')

print(f'Weight(Pytorch) : {W.detach().numpy()}')

print(f'Bias(Pytorch) : {B.detach().numpy()}')

```


y(Pytorch) : [[0.9213998]]



Weight(Pytorch) : [[0.28007793]]



Bias(Pytorch) : [-0.3392594]


### numpy


```python

import numpy as np



z = 10.0 * 0.28007793 - 0.3392594

y = 1 / (1 + np.exp(-z))

print(y)

```


0.9213998078592858


## x가 p개의 변수로 구성되고(다변량), 뉴런이 1개인 경우


### pytorch


```python

import torch



x = torch.tensor([[10., 5., 3., 1.]])



Linear = torch.nn.Linear(in_features = 4, out_features = 1)

Sigmoid = torch.nn.Sigmoid()



y = Sigmoid(Linear(x))



W = Linear.weight

B = Linear.bias



print(f'y(Pytorch) : {y.detach().numpy()}')

print(f'Weight(Pytorch) : {W.detach().numpy()}')

print(f'Bias(Pytorch) : {B.detach().numpy()}')

```


y(Pytorch) : [[0.0026371]]



Weight(Pytorch) : [[-0.49273372 -0.2617575   0.01708454  0.44085765]]



Bias(Pytorch) : [-0.19142264]


### numpy


```python

import numpy as np



x_T = x.detach().numpy()

W_T = W.detach().numpy()



z = np.matmul(x_T, W_T.transpose()) - 0.19142264

y = 1 / (1 + np.exp(-z))

print(y)

```


[[0.0026371]]


## 다변량 x가 여러개이고(sample size가 n개), 뉴런이 1개인 경우


### pytorch


```python

import torch



x = torch.rand((4, 2))



Linear = torch.nn.Linear(in_features = 2, out_features = 1)

Sigmoid = torch.nn.Sigmoid()



y = Sigmoid(Linear(x))



W = Linear.weight

B = Linear.bias



print(f'y(Pytorch) : \n {y.detach().numpy()}')

print(f'Weight(Pytorch) : {W.detach().numpy()}')

print(f'Bias(Pytorch) : {B.detach().numpy()}')

```


y(Pytorch) : 



 [[0.68523717]

 

 [0.72653574]

 

 [0.66456914]

 

 [0.5666909 ]]

 

Weight(Pytorch) : [[0.6898143  0.37645656]]



Bias(Pytorch) : [0.03636498]


### numpy


```python

import numpy as np



x_T = x.detach().numpy()

W_T = W.detach().numpy()



z = np.matmul(x_T, W_T.transpose()) + B.detach().numpy()

y = 1 / (1 + np.exp(-z))

print(y)

```


[[0.68523717]



 [0.72653574]

 

 [0.66456914]

 

 [0.5666909 ]]


## shape


torch.nn.Linear()에는 in_features로 입력값의 차원(또는 이전 층의 노드 갯수)을, out_feature로 출력값의 차원(Layer에서 노드의 갯수)를 입력하면 되므로 차원계산이 쉽다.



pytorch 기준으로

노드 갯수가 고정된 상태에서 입력값의 차원 증가(1, p)는 weight의 차원증가(1, p)를 이끌지만 다른것들(y, bias)의 차원에는 영향을 미치지 않는다.



노드 갯수가 고정된 상태에서 입력값의 개수증가(n, p)는 y값의 개수 증가(n, 1)를 이끌지만 다른것들(weight, bias)의 차원 및 개수에는 영향을 미치지 않는다.

