{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8ca740",
   "metadata": {},
   "source": [
    "# Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3b0c6",
   "metadata": {},
   "source": [
    "각 뉴런들은 서로 다른 weight와 bias를 가진 parametric function으로, filtering 연산으로 볼 수 있다.\n",
    "따라서 여러개의 뉴런들을 묶은 하나의 층(Dense Layer)는 서로 다른 parametric funciton의 묶음\n",
    "\n",
    "Filter bank의 개념 : 같은 값을 넣더라도 다른값이 나온다.\n",
    "    \n",
    "Deep Learning : cascaded(layer가 쌓아짐) filter bank 구조\n",
    "\n",
    "보통 Dense Layer는 모든 노드가 연결된 fully connected layer를 말한다.\n",
    "\n",
    "network는 edge(connection)와 node로 구성된다.\n",
    "\n",
    "sigmoid 함수는 파라미터가 없지만, activation layer라고 부르기도 한다.\n",
    "\n",
    "각 Layer의 출력값은 다음 layer의 input 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad28aca",
   "metadata": {},
   "source": [
    "## shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02c584",
   "metadata": {},
   "source": [
    "**Pytorch 기준**\n",
    "\n",
    "X = (샘플수, 변수개수) 일때,\n",
    "\n",
    "Weight = (뉴런개수, 변수개수)\n",
    "\n",
    "Bias = (뉴런개수)\n",
    "\n",
    "Output = (샘플수, 뉴런개수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f5496",
   "metadata": {},
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c53640",
   "metadata": {},
   "source": [
    "### shapes of dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32e3392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (8, 10)\n",
      "Weight: (3, 10)\n",
      "Bias: (3,)\n",
      "y : (8, 3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, n_feature = 8, 10\n",
    "X = torch.rand(N, n_feature)\n",
    "\n",
    "n_neuron = 3\n",
    "Linear = torch.nn.Linear(in_features = n_feature, out_features = n_neuron)\n",
    "Sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "Y = Sigmoid(Linear(X))\n",
    "\n",
    "W = Linear.weight\n",
    "B = Linear.bias\n",
    "\n",
    "\n",
    "print(f'X: {X.detach().numpy().shape}')\n",
    "print(f'Weight: {W.detach().numpy().shape}')\n",
    "print(f'Bias: {B.detach().numpy().shape}')\n",
    "print(f'y : {Y.detach().numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91900e",
   "metadata": {},
   "source": [
    "### shapes of cascaded dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1decb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (4, 10) \n",
      "\n",
      "Weight1: (3, 10)\n",
      "Bias1: (3,)\n",
      "A1: (4, 3) \n",
      "\n",
      "Weight2 : (5, 3)\n",
      "Bias2 : (5,)\n",
      "Y : (4, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, n_feature = 4, 10\n",
    "X = torch.rand(N, n_feature)\n",
    "\n",
    "n_neurons = [3, 5]\n",
    "Linear1 = torch.nn.Linear(in_features = n_feature, out_features = n_neurons[0])\n",
    "Linear2 = torch.nn.Linear(in_features = n_neurons[0], out_features = n_neurons[1])\n",
    "Sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# forward propagation\n",
    "A1 = Sigmoid(Linear1(X))\n",
    "Y = Sigmoid(Linear2(A1))\n",
    "\n",
    "# get weight/bias\n",
    "W1 = Linear1.weight\n",
    "B1 = Linear1.bias\n",
    "\n",
    "W2 = Linear2.weight\n",
    "B2 = Linear2.bias\n",
    "\n",
    "print(f'X: {X.detach().numpy().shape} \\n')\n",
    "\n",
    "print(f'Weight1: {W1.detach().numpy().shape}')\n",
    "print(f'Bias1: {B1.detach().numpy().shape}')\n",
    "print(f'A1: {A1.detach().numpy().shape} \\n')\n",
    "\n",
    "print(f'Weight2 : {W2.detach().numpy().shape}')\n",
    "print(f'Bias2 : {B2.detach().numpy().shape}')\n",
    "print(f'Y : {Y.detach().numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db160ec",
   "metadata": {},
   "source": [
    "### dense layers with python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406c16b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : (4, 10)\n",
      "After dense layer  0\n",
      "(4, 10) \n",
      "\n",
      "After dense layer  1\n",
      "(4, 20) \n",
      "\n",
      "After dense layer  2\n",
      "(4, 30) \n",
      "\n",
      "After dense layer  3\n",
      "(4, 40) \n",
      "\n",
      "After dense layer  4\n",
      "(4, 50) \n",
      "\n",
      "After dense layer  5\n",
      "(4, 60) \n",
      "\n",
      "After dense layer  6\n",
      "(4, 70) \n",
      "\n",
      "After dense layer  7\n",
      "(4, 80) \n",
      "\n",
      "After dense layer  8\n",
      "(4, 90) \n",
      "\n",
      "After dense layer  9\n",
      "(4, 100) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, n_feature = 4, 10\n",
    "X = torch.rand(N, n_feature)\n",
    "\n",
    "n_neurons = [n_feature, 10, 20, 30 ,40, 50 ,60 ,70 ,80 ,90 ,100]\n",
    "\n",
    "ReLU = torch.nn.ReLU()\n",
    "\n",
    "dense_layers = []\n",
    "for i in range(0, len(n_neurons)-1):\n",
    "    Linear = torch.nn.Linear(in_features = n_neurons[i], out_features = n_neurons[i+1])\n",
    "    dense_layers.append(Linear)\n",
    "\n",
    "print(f'Input : {X.detach().numpy().shape}')\n",
    "for dense_idx, dense in enumerate(dense_layers):\n",
    "    X = dense(X)\n",
    "    X = ReLU(X)\n",
    "    print(\"After dense layer \", dense_idx)\n",
    "    print(X.detach().numpy().shape, '\\n')\n",
    "\n",
    "Y = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3d992",
   "metadata": {},
   "source": [
    "### Model Implementation with Sequential Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0e4e388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=4, out_features=5, bias=True)\n",
       "  (3): Sigmoid()\n",
       "  (4): Linear(in_features=5, out_features=6, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_neurons = [3, 4, 5, 6]\n",
    "\n",
    "Sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = []\n",
    "for i in range(0, len(n_neurons)-1):\n",
    "    model.append(torch.nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "    model.append(Sigmoid)\n",
    "\n",
    "model = torch.nn.Sequential(*model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e884e2",
   "metadata": {},
   "source": [
    "### Model implementation with Model-subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f55dcf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestModel(\n",
       "  (dense1): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (dense2): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (ReLU): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = torch.nn.Linear(n_feature, 10)\n",
    "        self.dense2 = torch.nn.Linear(10, 20)\n",
    "        \n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.ReLU(x)\n",
    "        return x\n",
    "    \n",
    "model = TestModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebc3c2",
   "metadata": {},
   "source": [
    "### forward propagation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4283d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1317, 0.2153, 0.4418, 0.0184, 0.0000, 0.0979, 0.0952, 0.1561, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0669, 0.0000, 0.0089, 0.1980,\n",
      "         0.0998, 0.2156],\n",
      "        [0.2318, 0.2630, 0.4788, 0.0390, 0.0000, 0.1292, 0.0000, 0.0487, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2188, 0.0000, 0.0552, 0.2272,\n",
      "         0.2534, 0.3283],\n",
      "        [0.1088, 0.2664, 0.4972, 0.0335, 0.0000, 0.1365, 0.0776, 0.1855, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0495, 0.0000, 0.0000, 0.1238,\n",
      "         0.1446, 0.2640],\n",
      "        [0.2265, 0.3517, 0.6527, 0.0000, 0.0000, 0.1875, 0.0295, 0.0852, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0091, 0.0000, 0.2791, 0.0000, 0.0291, 0.1006,\n",
      "         0.4052, 0.5522]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, n_feature = 4, 10\n",
    "X = torch.rand(N, n_feature)\n",
    "\n",
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense1 = torch.nn.Linear(n_feature, 10)\n",
    "        self.dense2 = torch.nn.Linear(10, 20)\n",
    "        \n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ReLU(self.dense1(x))\n",
    "        x = self.ReLU(self.dense2(x))\n",
    "        return x\n",
    "    \n",
    "model = TestModel()\n",
    "Y = model(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0d26b",
   "metadata": {},
   "source": [
    "### Layers in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6503a7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=4, out_features=5, bias=True)\n",
       "  (3): Sigmoid()\n",
       "  (4): Linear(in_features=5, out_features=6, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c60accd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3) (4,)\n",
      "(5, 4) (5,)\n",
      "(6, 5) (6,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_neurons = [3, 4, 5, 6]\n",
    "\n",
    "Sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = []\n",
    "for i in range(0, len(n_neurons)-1):\n",
    "    model.append(torch.nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "    model.append(Sigmoid)\n",
    "\n",
    "model = torch.nn.Sequential(*model)\n",
    "\n",
    "for i in range(0, len(model), 2):\n",
    "    W = model[i].weight\n",
    "    B = model[i].bias\n",
    "    \n",
    "    print(W.detach().numpy().shape, B.detach().numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841da99",
   "metadata": {},
   "source": [
    "### Trainable Variables in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "403caab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "25\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_neurons = [3, 4, 5, 6]\n",
    "\n",
    "Sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = []\n",
    "for i in range(0, len(n_neurons)-1):\n",
    "    model.append(torch.nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "    model.append(Sigmoid)\n",
    "\n",
    "model = torch.nn.Sequential(*model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for i in range(0, len(model), 2):\n",
    "    print(count_parameters(model[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
