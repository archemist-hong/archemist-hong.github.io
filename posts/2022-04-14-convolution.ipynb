{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bedf27",
   "metadata": {},
   "source": [
    "# Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dafb6b",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da59b3e",
   "metadata": {},
   "source": [
    "### 1개 채널"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de548b3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '이미지' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8472/2753060902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m이미지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '이미지' is not defined"
     ]
    }
   ],
   "source": [
    "이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7321862",
   "metadata": {},
   "source": [
    "### 여러개 채널"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d34777",
   "metadata": {},
   "source": [
    "## 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099c58dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '이미지' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8472/2753060902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m이미지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '이미지' is not defined"
     ]
    }
   ],
   "source": [
    "이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb079c6",
   "metadata": {},
   "source": [
    "## 특성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3584b4c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '이미지' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8472/2753060902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m이미지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '이미지' is not defined"
     ]
    }
   ],
   "source": [
    "이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d913c",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d117cf9",
   "metadata": {},
   "source": [
    "$O_{h} = floor({I_{h} - K_{h} + 2P \\over S} + 1)$\n",
    "\n",
    "$O_{w} = floor({I_{w} - K_{w} + 2P \\over S} + 1)$\n",
    "\n",
    "$O_{h}, O_{w} = 피처맵의 높이, 너비$\n",
    "\n",
    "$I_{h}, I_{w} = 인풋의 높이, 너비$\n",
    "\n",
    "$K_{h}, K_{w} = 커널의 높이, 너비$\n",
    "\n",
    "$P = 패딩 사이즈$\n",
    "\n",
    "$S = 스트라이드(보폭)$\n",
    "\n",
    "floor => 소수점 이하 버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76709ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8910cba",
   "metadata": {},
   "source": [
    "pytorch에서 conv layer를 사용하기 위해서는\n",
    "\n",
    "(배치사이즈, 채널수, 높이, 너비)의 차원으로 인풋 이미지를 준비해야 한다.\n",
    "\n",
    "[Conv Layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)의 argument로는 (in_channels, out_channels, kernel_size)를 넘겨준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84604eb",
   "metadata": {},
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e0a53",
   "metadata": {},
   "source": [
    "### Shape of Conv Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27198ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 28, 28])\n",
      "torch.Size([1, 5, 3, 3])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 1, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, I_h, I_w, C_i = 1, 28, 28, 5\n",
    "n_filter = 1\n",
    "k_size = 3\n",
    "\n",
    "images = torch.rand(N, C_i, I_h, I_w)\n",
    "conv = torch.nn.Conv2d(C_i, n_filter, kernel_size = k_size)\n",
    "\n",
    "y = conv(images)\n",
    "\n",
    "W = conv.weight\n",
    "B = conv.bias\n",
    "\n",
    "print(images.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0641b3",
   "metadata": {},
   "source": [
    "### Convolution Calculatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efec175a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(Pytorch) : \n",
      " [[-0.6482204  -0.4697213  -0.72254604]\n",
      " [-0.50705934 -0.38746536 -0.48394796]\n",
      " [-0.15853219 -0.5524138  -0.60327333]]\n",
      "Y(Manual): \n",
      " [[-0.64822042 -0.46972129 -0.7225461 ]\n",
      " [-0.50705934 -0.38746539 -0.4839479 ]\n",
      " [-0.15853217 -0.55241382 -0.60327339]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "N, I_h, I_w, C_i = 1, 5, 5, 1\n",
    "n_filter = 1\n",
    "k_size = 3\n",
    "\n",
    "images = torch.rand(N, C_i, I_h, I_w)\n",
    "conv = torch.nn.Conv2d(C_i, n_filter, kernel_size = k_size)\n",
    "\n",
    "y = conv(images)\n",
    "print(\"Y(Pytorch) : \\n\", y.detach().numpy().squeeze())\n",
    "W = conv.weight\n",
    "B = conv.bias\n",
    "\n",
    "images = images.numpy().squeeze()\n",
    "W = W.squeeze()\n",
    "y_man = np.zeros((I_h - k_size + 1, I_w - k_size + 1))\n",
    "for h in range(I_h - k_size + 1):\n",
    "    for w in range(I_w - k_size + 1):\n",
    "        window = images[h : h + k_size, w : w + k_size]\n",
    "        y_man[h, w] = np.sum(window * W.detach().numpy()) + B\n",
    "\n",
    "print(\"Y(Manual): \\n\", y_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46d39f",
   "metadata": {},
   "source": [
    "### Shapes of Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e526aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image : (32, 3, 28, 26)\n",
      "W / B : (5, 3, 3, 3) / (5,)\n",
      "Output image : (32, 5, 26, 24)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, I_h, I_w, C_i = 32, 28, 26, 3\n",
    "n_filter = 5\n",
    "k_size = 3\n",
    "\n",
    "images = torch.rand(N, C_i, I_h, I_w)\n",
    "conv = torch.nn.Conv2d(C_i, n_filter, kernel_size = k_size)\n",
    "\n",
    "y = conv(images)\n",
    "\n",
    "W = conv.weight\n",
    "B = conv.bias\n",
    "\n",
    "print(\"Input image : {}\".format(images.detach().numpy().shape))\n",
    "print(\"W / B : {} / {}\".format(W.detach().numpy().shape, B.detach().numpy().shape))\n",
    "print(\"Output image : {}\".format(y.detach().numpy().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65366ce1",
   "metadata": {},
   "source": [
    "### Conv Layers with Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27c2b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(Pytorch) : \n",
      " [[[0.44595766 0.4009004 ]\n",
      "  [0.36888936 0.41369557]]\n",
      "\n",
      " [[0.5639003  0.57518417]\n",
      "  [0.48839545 0.49236757]]\n",
      "\n",
      " [[0.5087737  0.5014534 ]\n",
      "  [0.4796335  0.49631336]]]\n",
      "Y(Manual): \n",
      " [[[0.44595766 0.40090035]\n",
      "  [0.36888932 0.41369559]]\n",
      "\n",
      " [[0.5639003  0.57518422]\n",
      "  [0.48839545 0.49236754]]\n",
      "\n",
      " [[0.50877369 0.50145337]\n",
      "  [0.47963357 0.49631336]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "N, I_h, I_w, C_i = 1, 5, 5, 3\n",
    "n_filter = 3\n",
    "k_size = 4\n",
    "\n",
    "images = torch.rand(N, C_i, I_h, I_w)\n",
    "\n",
    "# Forward Propagation (Pytorch)\n",
    "conv = torch.nn.Conv2d(C_i, n_filter, kernel_size = k_size)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "y = conv(images)\n",
    "y = sigmoid(y)\n",
    "y = y.detach().numpy().squeeze()\n",
    "print(\"Y(Pytorch) : \\n\", y)\n",
    "\n",
    "W = conv.weight\n",
    "B = conv.bias\n",
    "\n",
    "# Forward Propagation (Manual)\n",
    "images = images.numpy().squeeze()\n",
    "y_man = np.zeros((n_filter, I_h - k_size + 1, I_w - k_size + 1))\n",
    "\n",
    "for c in range(n_filter):\n",
    "    c_W = W[c, :, :, :]\n",
    "    c_b = B[c]\n",
    "    for h in range(I_h - k_size + 1):\n",
    "        for w in range(I_w - k_size + 1):\n",
    "            window = images[:, h : h + k_size, w : w + k_size]\n",
    "            conv = np.sum(window * c_W.detach().numpy()) + c_b.detach().numpy()\n",
    "            conv = 1 / (1 + np.exp(-conv))\n",
    "            \n",
    "            y_man[c, h, w] = conv\n",
    "\n",
    "print(\"Y(Manual): \\n\", y_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e31419",
   "metadata": {},
   "source": [
    "### Models with Sequential Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "445cb7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 3, 3) (10,)\n",
      "(20, 10, 3, 3) (20,)\n",
      "(30, 20, 3, 3) (30,)\n",
      "======\n",
      "torch.Size([10, 3, 3, 3])\n",
      "torch.Size([10])\n",
      "torch.Size([20, 10, 3, 3])\n",
      "torch.Size([20])\n",
      "torch.Size([30, 20, 3, 3])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_neurons = [10, 20, 30]\n",
    "\n",
    "model = torch.nn.Sequential()\n",
    "model.add_module('conv1',torch.nn.Conv2d(3, n_neurons[0],  kernel_size = 3))\n",
    "model.add_module('relu1', torch.nn.ReLU())\n",
    "model.add_module('conv2',torch.nn.Conv2d(n_neurons[0], n_neurons[1], kernel_size = 3))\n",
    "model.add_module('relu2',torch.nn.ReLU())\n",
    "model.add_module('conv3',torch.nn.Conv2d(n_neurons[1], n_neurons[2], kernel_size = 3))\n",
    "model.add_module('relu3',torch.nn.ReLU())\n",
    "\n",
    "x = torch.randn(32, 3, 28, 298)\n",
    "predictions = model(x)\n",
    "\n",
    "for layer in range(0, len(model), 2):\n",
    "    W = model[layer].weight\n",
    "    B = model[layer].bias\n",
    "    print(W.detach().numpy().shape, B.detach().numpy().shape)\n",
    "    \n",
    "print('======')\n",
    "\n",
    "parameters = model.parameters()\n",
    "for param in parameters:\n",
    "    print(param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354973c9",
   "metadata": {},
   "source": [
    "### Models with Model Sub-classing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe2abc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (32, 3, 28, 28)\n",
      "Output: (32, 30, 22, 22)\n",
      "(10, 3, 3, 3) (10,)\n",
      "(20, 10, 3, 3) (20,)\n",
      "(30, 20, 3, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_neurons = [3, 10, 20, 30]\n",
    "\n",
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        global n_neurons\n",
    "        \n",
    "        self.conv_layers = []\n",
    "        for i in range(0, len(n_neurons) - 1):\n",
    "            self.conv_layers.append(torch.nn.Conv2d(n_neurons[i], n_neurons[i + 1], kernel_size = 3))\n",
    "            self.conv_layers.append(torch.nn.ReLU())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        return x\n",
    "        \n",
    "model = TestModel()\n",
    "x = torch.randn(32, 3, 28, 28)\n",
    "predictions = model(x)\n",
    "\n",
    "print(f\"Input: {x.detach().numpy().shape}\")\n",
    "print(f\"Output: {predictions.detach().numpy().shape}\")\n",
    "\n",
    "for layer in range(0, len(model.conv_layers), 2):\n",
    "    W = model.conv_layers[layer].weight\n",
    "    B = model.conv_layers[layer].bias\n",
    "    print(W.detach().numpy().shape, B.detach().numpy().shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
